# Steps to deploy multi-node LLM using Triton + TRT-LLM on EKS cluster

## 1. Build the custom container image and push it to Amazon ECR

We need to build a custom image on top of Triton TRT-LLM NGC container to include the kubessh file, server.py, and other EFA libraries and will then push this image to Amazon ECR. You can take a look at the [Dockerfile here](https://github.com/Wenhan-Tan/EKS_Multinode_Triton_TRTLLM/blob/main/multinode_helm_chart/containers/triton_trt_llm.containerfile).

```
## AWS
export AWS_REGION=us-east-1
export ACCOUNT=$(aws sts get-caller-identity --query Account --output text)

## Docker Image
export REGISTRY=${ACCOUNT}.dkr.ecr.${AWS_REGION}.amazonaws.com/
export IMAGE=triton_trtllm_multinode
export TAG=":24.07"

docker build \
  --file ./triton_trt_llm.containerfile \
  --rm \
  --tag ${REGISTRY}${IMAGE}${TAG} \
  .

echo "Logging in to $REGISTRY ..."
aws ecr get-login-password | docker login --username AWS --password-stdin $REGISTRY

# Create registry if it does not exist
REGISTRY_COUNT=$(aws ecr describe-repositories | grep ${IMAGE} | wc -l)
if [ "$REGISTRY_COUNT" == "0" ]; then
        echo ""
        echo "Creating repository ${IMAGE} ..."
        aws ecr create-repository --repository-name ${IMAGE}
fi

# Push image
docker image push ${REGISTRY}${IMAGE}${TAG}
```

## 2. Setup Triton model repository for LLM deployment:

To build the TRT-LLM engine and set up Triton model repository inside the compute node use the following steps:

### a. Modify the `setup_ssh_efs.yaml` file

We use the `setup_ssh_efs.yaml` file which does "sleep infinity" to set up ssh access inside the compute node along with EFS.

Adjust the following values:

- `image`: change image tag. Default is 24.07 which supports TRT-LLM v0.11.0
- `nvidia.com/gpu`: set to the number of GPUs per node in your cluster, adjust in both the limits and requests section
- `claimName`: set to your EFS pvc name

### b. SSH into compute node and build TRT-LLM engine

Deploy the pod:

```
cd multinode_helm_chart/
kubectl apply -f setup_ssh_efs.yaml
kubectl exec -it setup_ssh_efs -- bash
```

Clone the Triton TRT-LLM backend repository:

```
cd <EFS_mount_path>
git clone https://github.com/triton-inference-server/tensorrtllm_backend.git -b v0.11.0
cd tensorrtllm_backend
git lfs install
git submodule update --init --recursive
```

Build a Llama3-8B engine with Tensor Parallelism=4, Pipeline Parallelism=2 to run on 2 nodes of g5.12xlarge (4 A10G GPUs each), so total of 8 GPUs across 2 nodes.

```
cd tensorrtllm_backend/tensorrt_llm/examples/llama
git clone https://huggingface.co/meta-llama/Meta-Llama-3-8B

python convert_checkpoint.py --model_dir ./Meta-Llama-3-8B \
                             --output_dir ./converted_checkpoint \
                             --dtype float16 \
                             --tp_size 4 \
                             --pp_size 2

trtllm-build --checkpoint_dir ./converted_checkpoint \
             --output_dir ./output_engines \
             --gemm_plugin float16 \
             --use_custom_all_reduce disable \ # only disable on non-NVLink machines like g5.12xlarge
             --max_input_len 2048 \
             --max_output_len 2048 \
             --max_batch_size 4 
```

### c. Prepare the Triton model repository

```
cd <EFS_MOUNT_PATH>/tensorrtllm_backend
mkdir triton_model_repo

cp -r all_models/inflight_batcher_llm/ensemble triton_model_repo/
cp -r all_models/inflight_batcher_llm/preprocessing triton_model_repo/
cp -r all_models/inflight_batcher_llm/postprocessing triton_model_repo/
cp -r all_models/inflight_batcher_llm/tensorrt_llm triton_model_repo/

python3 tools/fill_template.py -i triton_model_repo/preprocessing/config.pbtxt tokenizer_dir:<PATH_TO_TOKENIZER>,tokenizer_type:llama,triton_max_batch_size:4,preprocessing_instance_count:1
python3 tools/fill_template.py -i triton_model_repo/tensorrt_llm/config.pbtxt triton_backend:tensorrtllm,triton_max_batch_size:4,decoupled_mode:True,max_beam_width:1,engine_dir:<PATH_TO_ENGINES>,,enable_kv_cache_reuse:False,batching_strategy:inflight_batching,max_queue_delay_microseconds:0
python3 tools/fill_template.py -i triton_model_repo/postprocessing/config.pbtxt tokenizer_dir:<PATH_TO_TOKENIZER>,tokenizer_type:llama,triton_max_batch_size:4,postprocessing_instance_count:1
python3 tools/fill_template.py -i triton_model_repo/ensemble/config.pbtxt triton_max_batch_size:4
```

> [!Note]
> Be sure to substitute the correct values for `<PATH_TO_TOKENIZER>` and `<PATH_TO_ENGINES>` in the example above. Keep in mind that the tokenizer, the TRT-LLM engines, and the Triton model repository shoudl be in a shared file storage between your nodes. They're required to launch your model in Triton. For example, if using AWS EFS, the values for `<PATH_TO_TOKENIZER>` and `<PATH_TO_ENGINES>` should be respect to the actutal EFS mount path. This is determined by your persistent-volume claim and mount path in chart/templates/deployment.yaml. Make sure that your nodes are able to access these files.

## 3. Create `example_values.yaml` file for deployment

Make sure you go over the provided `values.yaml` first to understand what each value represents.

Below is the `example_values.yaml` file we use where `<EFS_MOUNT_PATH>=/var/run/models`:

```
gpu: NVIDIA-A10G
gpuPerNode: 4
persistentVolumeClaim: efs-claim

tensorrtLLM:
  parallelism:
    tensor: 4
    pipeline: 2

triton:
  image:
    name: wenhant16/triton_trtllm_multinode:24.07.10
    # name: ${ACCOUNT}.dkr.ecr.${AWS_REGION}.amazonaws.com/triton_trtllm_multinode:24.07
  resources:
    cpu: 4
    memory: 64Gi
    efa: 1 # If you don't want to enable EFA, set this to 0.
  triton_model_repo_path: /var/run/models/tensorrtllm_backend/triton_model_repo
  enable_nsys: false # Note if you send lots of requests, nsys report can be very large.

logging:
  tritonServer:
    verbose: False

autoscaling:
  enable: true
  replicas:
    maximum: 2
    minimum: 1
  metric:
    name: triton:queue_compute:ratio
    value: 1
```

## 4. Install the Helm chart

```
helm install multinode_deployment \
  --values ./chart/values.yaml \
  --values ./chart/example_values.yaml \
  ./chart/.
```

In this example, we are going to deploy Triton server on 2 nodes with 4 GPUs each. This will result in having 2 pods running in your cluster. Command `kubectl get pods` should output something similar to below:

```
NAME                         READY   STATUS    RESTARTS   AGE
leaderworkerset-sample-0     1/1     Running   0          28m
leaderworkerset-sample-0-1   1/1     Running   0          28m
```

Use the following command to check Triton logs:

```
kubectl logs --follow leaderworkerset-sample-0
```

You should output something similar to below:

```
I0717 23:01:28.501008 300 server.cc:674] 
+----------------+---------+--------+
| Model          | Version | Status |
+----------------+---------+--------+
| ensemble       | 1       | READY  |
| postprocessing | 1       | READY  |
| preprocessing  | 1       | READY  |
| tensorrt_llm   | 1       | READY  |
+----------------+---------+--------+

I0717 23:01:28.501073 300 tritonserver.cc:2579] 
+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Option                           | Value                                                                                                                                                                                                           |
+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| server_id                        | rank0                                                                                                                                                                                                           |
| server_version                   | 2.47.0                                                                                                                                                                                                          |
| server_extensions                | classification sequence model_repository model_repository(unload_dependents) schedule_policy model_configuration system_shared_memory cuda_shared_memory binary_tensor_data parameters statistics trace logging |
| model_repository_path[0]         | /var/run/models/tensorrtllm_backend/triton_model_repo                                                                                                                                                          |
| model_control_mode               | MODE_NONE                                                                                                                                                                                                       |
| strict_model_config              | 1                                                                                                                                                                                                               |
| model_config_name                |                                                                                                                                                                                                                 |
| rate_limit                       | OFF                                                                                                                                                                                                             |
| pinned_memory_pool_byte_size     | 268435456                                                                                                                                                                                                       |
| cuda_memory_pool_byte_size{0}    | 67108864                                                                                                                                                                                                        |
| min_supported_compute_capability | 6.0                                                                                                                                                                                                             |
| strict_readiness                 | 1                                                                                                                                                                                                               |
| exit_timeout                     | 30                                                                                                                                                                                                              |
| cache_enabled                    | 0                                                                                                                                                                                                               |
+----------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

I0717 23:01:28.502835 300 grpc_server.cc:2463] "Started GRPCInferenceService at 0.0.0.0:8001"
I0717 23:01:28.503047 300 http_server.cc:4692] "Started HTTPService at 0.0.0.0:8000"
I0717 23:01:28.544321 300 http_server.cc:362] "Started Metrics Service at 0.0.0.0:8002"
```

> [!Note]
> You may run into an error of `the GPU number is incompatible with 8 gpusPerNode when MPI size is 8`. The root cause is starting from v0.11.0, TRT-LLM backend checks the gpusPerNode parameter in the `config.json` file inside the output engines folder. This parameter is set during engine build time. If the value is the not the same as the number of GPUs in your node, this assertion error shows up. To resolve this, simply change the value in the file to match the number of GPUs in your node.

## 5. Send a Curl POST request for infernce

In this AWS example, we can view the external IP address of Load Balancer by running `kubectl get services`. Note that we use `multinode_deployment` as helm chart installation name here. Your output should look something similar to below:

```
NAME                     TYPE           CLUSTER-IP      EXTERNAL-IP                                                              PORT(S)                                        AGE
kubernetes               ClusterIP      10.100.0.1      <none>                                                                   443/TCP                                        43d
leaderworkerset-sample   ClusterIP      None            <none>                                                                   <none>                                         54m
multinode_deployment     LoadBalancer   10.100.44.170   a69c447a535104f088d2e924f5523d41-634913838.us-east-1.elb.amazonaws.com   8000:32120/TCP,8001:32263/TCP,8002:31957/TCP   54m
```

You can send a CURL request to the `ensemble` TRT-LLM Llama-3 model hosted in Triton Server with the following command:

```
curl -X POST a69c447a535104f088d2e924f5523d41-634913838.us-east-1.elb.amazonaws.com:8000/v2/models/ensemble/generate -d '{"text_input": "What is machine learning?", "max_tokens": 64, "bad_words": "", "stop_words": "", "pad_id": 2, "end_id": 2}'
```

You should output similar to below:

```
{"context_logits":0.0,"cum_log_probs":0.0,"generation_logits":0.0,"model_name":"ensemble","model_version":"1","output_log_probs":[0.0,0.0,0.0,0.0,0.0],"sequence_end":false,"sequence_id":0,"sequence_start":false,"text_output":" Machine learning is a branch of artificial intelligence that deals with the development of algorithms that allow computers to learn from data and make predictions or decisions without being explicitly programmed. Machine learning algorithms are used in a wide range of applications, including image recognition, natural language processing, and predictive analytics.\nWhat is the difference between machine learning and"}
```

> [!Note]
> You may run into an error of `Multiple tagged security groups found for instance i-*************`. The root cause is both EKS cluster security group and EFA security group are using the same tag of `kubernetes.io/cluster/wenhant-eks-cluster : owned`. This tag should only be attached to 1 security group, usually your main security group. To resolve this, simply delete the tag from the EFA security group.

## 6. Test Horizontal Pod Autoscaler and Cluster Autoscaler

To check HPA status, run:

```
kubectl get hpa multinode_deployment
```

You should output something similar to below:

```
NAME                   REFERENCE                                TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
multinode_deployment   LeaderWorkerSet/leaderworkerset-sample   0/1       1         2         1          66m
```

From the output above, the current metric value is 0 and the target value is 1. Note that in this example, our metric is a custom metric defined in Prometheus Rule. You can find more details in the [Install Prometheus rule for Triton metrics](Cluster_Setup_Steps.md#8-install-prometheus-rule-for-triton-metrics) step. When the current value exceed 1, the HPA will start to create a new replica. We can either increase traffic by sending a large amount of requests to the LoadBalancer or manually increase minimum number of replicas to let the HPA create the second replica. In this example, we are going to choose the latter and run the following command:

```
kubectl patch hpa multinode_deployment -p '{"spec":{"minReplicas": 2}}'
```

Your `kubectl get pods` command should output something similar to below:

```
NAME                         READY   STATUS    RESTARTS   AGE
leaderworkerset-sample-0     1/1     Running   0          6h48m
leaderworkerset-sample-0-1   1/1     Running   0          6h48m
leaderworkerset-sample-1     0/1     Pending   0          13s
leaderworkerset-sample-1-1   0/1     Pending   0          13s
```

Here we can see the second replica is created but in `Pending` status. If you run `kubectl describe pod leaderworkerset-sample-1`, you should see events similar to below:

```
Events:
  Type     Reason            Age   From                Message
  ----     ------            ----  ----                -------
  Warning  FailedScheduling  48s   default-scheduler   0/3 nodes are available: 1 node(s) didn't match Pod's node affinity/selector, 2 Insufficient nvidia.com/gpu, 2 Insufficient vpc.amazonaws.com/efa. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod.
  Normal   TriggeredScaleUp  15s   cluster-autoscaler  pod triggered scale-up: [{eks-efa-compute-ng-2-7ac8948c-e79a-9ad8-f27f-70bf073a9bfa 2->4 (max: 4)}]
```

The first event means that there are no available nodes to schedule any pods. This explains why the second 2 pods are in `Pending` status. The second event states that the Cluster Autoscaler detects that this pod is `unschedulable`, so it is going to increase number of nodes in our cluster until maximum is reached. You can find more details in the [Install Cluster Autoscaler](Cluster_Setup_Steps.md#10-install-cluster-autoscaler) step. This process can take some time depending on whether AWS have enough nodes available to add to your cluster. Eventually, the Cluster Autoscaler will add 2 more nodes in your node group so that the 2 `Pending` pods can be scheduled on them. Your `kubectl get nodes` and `kubectl get pods` commands should output something similar to below:

```
NAME                             STATUS   ROLES    AGE   VERSION
ip-192-168-103-11.ec2.internal   Ready    <none>   15m   v1.30.2-eks-1552ad0
ip-192-168-106-8.ec2.internal    Ready    <none>   15m   v1.30.2-eks-1552ad0
ip-192-168-117-30.ec2.internal   Ready    <none>   11h   v1.30.2-eks-1552ad0
ip-192-168-127-31.ec2.internal   Ready    <none>   11h   v1.30.2-eks-1552ad0
ip-192-168-26-106.ec2.internal   Ready    <none>   11h   v1.30.2-eks-1552ad0
```

```
leaderworkerset-sample-0     1/1     Running   0          7h26m
leaderworkerset-sample-0-1   1/1     Running   0          7h26m
leaderworkerset-sample-1     1/1     Running   0          38m
leaderworkerset-sample-1-1   1/1     Running   0          38m
```

You can run the following command to change minimum replica back to 1:

```
kubectl patch hpa multinode_deployment -p '{"spec":{"minReplicas": 1}}'
```

The HPA will delete the second replica if current metric does not exceed the target value. The Cluster Autoscaler will also remove the added 2 nodes when it detects them as "free".

## 7. Uninstall the Helm chart

```
helm uninstall <installation_name>
```

## 8. (Optional) NCCL Test

To test whether EFA is working properly, we can run a NCCL test across nodes. Make sure you modify the [nccl_test.yaml](./multinode_helm_chart/nccl_test.yaml) file and adjust the following values:

- `slotsPerWorker`: set to the number of GPUs per node in your cluster
- `-np`: set to "number_of_worker_nodes" * "number_of_gpus_per_node"
- `-N`: set to number_of_gpus_per_node
- `Worker: replicas`: set to number of worker pods you would like the test to run on. This must be less than or eaqual to the number of nodes in your cluster
- `node.kubernetes.io/instance-type`: set to the instance type of the nodes in your cluster against which you would like the nccl test to be run
- `nvidia.com/gpu`: set to the number of GPUs per node in your cluster, adjust in both the limits and requests section
- `vpc.amazonaws.com/efa`: set to the number of EFA adapters per node in your cluster, adjust in both the limits and requests section

Run the command below to deploy the MPI Operator which is required by the NCCL Test manifest:

```
kubectl apply --server-side -f https://raw.githubusercontent.com/kubeflow/mpi-operator/v0.5.0/deploy/v2beta1/mpi-operator.yaml
```

Run the command below to deploy NCCL test:

```
kubectl apply -f nccl_test.yaml
```

Note that the launcher pod will keep restarting until the connection is established with the worker pods. Run the command below to see the launcher pod logs:

```
kubectl logs -f $(kubectl get pods | grep launcher | cut -d ' ' -f 1)
```

You should output something similar to below (example of 2 x g5.12xlarge):

```
[1,0]<stdout>:#                                                              out-of-place                       in-place          
[1,0]<stdout>:#       size         count      type   redop    root     time   algbw   busbw #wrong     time   algbw   busbw #wrong
[1,0]<stdout>:#        (B)    (elements)                               (us)  (GB/s)  (GB/s)            (us)  (GB/s)  (GB/s)       
[1,0]<stdout>:           8             2     float     sum      -1[1,0]<stdout>:    99.10    0.00    0.00      0[1,0]<stdout>:    100.6    0.00    0.00      0
[1,0]<stdout>:          16             4     float     sum      -1[1,0]<stdout>:    103.4    0.00    0.00      0[1,0]<stdout>:    102.5    0.00    0.00      0
[1,0]<stdout>:          32             8     float     sum      -1[1,0]<stdout>:    103.5    0.00    0.00      0[1,0]<stdout>:    102.5    0.00    0.00      0
[1,0]<stdout>:          64            16     float     sum      -1[1,0]<stdout>:    103.6    0.00    0.00      0[1,0]<stdout>:    102.3    0.00    0.00      0
[1,0]<stdout>:         128            32     float     sum      -1[1,0]<stdout>:    103.8    0.00    0.00      0[1,0]<stdout>:    103.1    0.00    0.00      0
[1,0]<stdout>:         256            64     float     sum      -1[1,0]<stdout>:    103.9    0.00    0.00      0[1,0]<stdout>:    103.3    0.00    0.00      0
[1,0]<stdout>:         512           128     float     sum      -1[1,0]<stdout>:    104.3    0.00    0.01      0[1,0]<stdout>:    102.9    0.00    0.01      0
[1,0]<stdout>:        1024           256     float     sum      -1[1,0]<stdout>:    105.8    0.01    0.02      0[1,0]<stdout>:    104.9    0.01    0.02      0
[1,0]<stdout>:        2048           512     float     sum      -1[1,0]<stdout>:    116.4    0.02    0.03      0[1,0]<stdout>:    115.5    0.02    0.03      0
[1,0]<stdout>:        4096          1024     float     sum      -1[1,0]<stdout>:    120.4    0.03    0.06      0[1,0]<stdout>:    119.0    0.03    0.06      0
[1,0]<stdout>:        8192          2048     float     sum      -1[1,0]<stdout>:    134.2    0.06    0.11      0[1,0]<stdout>:    134.6    0.06    0.11      0
[1,0]<stdout>:       16384          4096     float     sum      -1[1,0]<stdout>:    147.9    0.11    0.19      0[1,0]<stdout>:    147.3    0.11    0.19      0
[1,0]<stdout>:       32768          8192     float     sum      -1[1,0]<stdout>:    182.3    0.18    0.31      0[1,0]<stdout>:    183.1    0.18    0.31      0
[1,0]<stdout>:       65536         16384     float     sum      -1[1,0]<stdout>:    194.6    0.34    0.59      0[1,0]<stdout>:    193.5    0.34    0.59      0
[1,0]<stdout>:      131072         32768     float     sum      -1[1,0]<stdout>:    267.5    0.49    0.86      0[1,0]<stdout>:    266.3    0.49    0.86      0
[1,0]<stdout>:      262144         65536     float     sum      -1[1,0]<stdout>:    495.7    0.53    0.93      0[1,0]<stdout>:    496.6    0.53    0.92      0
[1,0]<stdout>:      524288        131072     float     sum      -1[1,0]<stdout>:    746.2    0.70    1.23      0[1,0]<stdout>:    736.2    0.71    1.25      0
[1,0]<stdout>:     1048576        262144     float     sum      -1[1,0]<stdout>:   1337.1    0.78    1.37      0[1,0]<stdout>:   1333.2    0.79    1.38      0
[1,0]<stdout>:     2097152        524288     float     sum      -1[1,0]<stdout>:   2542.1    0.82    1.44      0[1,0]<stdout>:   2540.8    0.83    1.44      0
[1,0]<stdout>:     4194304       1048576     float     sum      -1[1,0]<stdout>:   3377.7    1.24    2.17      0[1,0]<stdout>:   3381.8    1.24    2.17      0
[1,0]<stdout>:     8388608       2097152     float     sum      -1[1,0]<stdout>:   5370.6    1.56    2.73      0[1,0]<stdout>:   5363.3    1.56    2.74      0
[1,0]<stdout>:    16777216       4194304     float     sum      -1[1,0]<stdout>:   9547.6    1.76    3.08      0[1,0]<stdout>:   9578.5    1.75    3.07      0
[1,0]<stdout>:    33554432       8388608     float     sum      -1[1,0]<stdout>:    17590    1.91    3.34      0[1,0]<stdout>:    17605    1.91    3.34      0
[1,0]<stdout>:    67108864      16777216     float     sum      -1[1,0]<stdout>:    34096    1.97    3.44      0[1,0]<stdout>:    34121    1.97    3.44      0
[1,0]<stdout>:   134217728      33554432     float     sum      -1[1,0]<stdout>:    67100    2.00    3.50      0[1,0]<stdout>:    67259    2.00    3.49      0
[1,0]<stdout>:   268435456      67108864     float     sum      -1[1,0]<stdout>:   133445    2.01    3.52      0[1,0]<stdout>:   133455    2.01    3.52      0
[1,0]<stdout>:   536870912     134217728     float     sum      -1[1,0]<stdout>:   266505    2.01    3.53      0[1,0]<stdout>:   266527    2.01    3.53      0
[1,0]<stdout>:  1073741824     268435456     float     sum      -1[1,0]<stdout>:   536019    2.00    3.51      0[1,0]<stdout>:   535942    2.00    3.51      0
[1,0]<stdout>:  2147483648     536870912     float     sum      -1[1,0]<stdout>:  1079960    1.99    3.48      0[1,0]<stdout>:  1079922    1.99    3.48      0
[1,0]<stdout>:  4294967296    1073741824     float     sum      -1[1,0]<stdout>:  2271140    1.89    3.31      0[1,0]<stdout>:  2268693    1.89    3.31      0
[1,0]<stdout>:# Out of bounds values : 0 OK
[1,0]<stdout>:# Avg bus bandwidth    : 1.42557
```

## 9. (Optional) GenAI-Perf

GenAI-Perf is a benchmarking tool for Triton server to measure latency and throughput of LLMs. We provide an example here.

### a. Modify the `gen_ai_perf.yaml` file

Adjust the following values:

- `image`: change image tag. Default is 24.07 which supports TRT-LLM v0.11.0
- `claimName`: set to your EFS pvc name

### b. Run benchmark

Run the below command to start a Triton server SDK container:

```
kubectl apply -f gen_ai_perf.yaml
kubectl exec -it gen-ai-perf -- bash
```

Run the below command to start benchmarking:

```
genai-perf \
  -m ensemble \
  --service-kind triton \
  --backend tensorrtllm \
  --num-prompts 100 \
  --random-seed 123 \
  --synthetic-input-tokens-mean 1024 \
  --synthetic-input-tokens-stddev 0 \
  --streaming \
  --output-tokens-mean 1024 \
  --output-tokens-stddev 0 \
  --output-tokens-mean-deterministic \
  --tokenizer hf-internal-testing/llama-tokenizer \
  --concurrency 1 \
  --measurement-interval 10000 \
  --url a69c447a535104f088d2e924f5523d41-634913838.us-east-1.elb.amazonaws.com:8001 \
  -- --request-count=10
```

You should output something similar to below (example of Mixtral 8x7B on 2 x g5.12xlarge):

```
                                            LLM Metrics                                             
┏━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━┳━━━━━━━━━━━┓
┃                Statistic ┃       avg ┃       min ┃       max ┃       p99 ┃       p90 ┃       p75 ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━╇━━━━━━━━━━━┩
│ Time to first token (ms) │    675.08 │    459.99 │  2,473.49 │  2,294.37 │    682.23 │    482.85 │
│ Inter token latency (ms) │     22.86 │     19.98 │     24.37 │     24.32 │     23.79 │     23.41 │
│     Request latency (ms) │ 29,906.05 │ 29,675.12 │ 31,814.10 │ 31,624.46 │ 29,917.75 │ 29,706.24 │
│   Output sequence length │  1,282.70 │  1,200.00 │  1,463.00 │  1,448.24 │  1,315.40 │  1,291.75 │
│    Input sequence length │  1,024.00 │  1,024.00 │  1,024.00 │  1,024.00 │  1,024.00 │  1,024.00 │
└──────────────────────────┴───────────┴───────────┴───────────┴───────────┴───────────┴───────────┘
Output token throughput (per sec): 42.89
Request throughput (per sec): 0.03
```
