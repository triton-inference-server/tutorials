# Copyright (c) 2024, NVIDIA CORPORATION. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Value must match the node's `.metadata.labels.nvidia.com/gpu.product` label.
# Run 'kubectl get nodes' to find node names.
# Run 'kubectl describe node <node_name>' to inspect a node's labels.
# Example values: NVIDIA-A100-SXM4-40GB, NVIDIA-A10G, Tesla-V100-SXM2-16GB, Tesla-T4
gpu: # (required)

# Example values: 1, 4, 8
gpuPerNode: # (required)

# Persistent volume claim where model content will be persisted.
# Expected to support read/write many access.
persistentVolumeClaim: # (required)

# Name of the secret to pull image
pullSecret: # (optional)

# Configuration options related to the AI model to be deployed.
tensorrtLLM: # (optional)
    # TensorRT-LLM format, specifically if/how the model is partitioned for deployment to multiple GPUs.
    parallelism: # (optional)
      # Pipeline parallelism involves sharding the model (vertically) into chunks, where each chunk comprises a
      # subset of layers that is executed on a separate device.
      # The main limitation of this method is that, due to the sequential nature of the processing, some devices or
      # layers may remain idle while waiting for the output.
      pipeline: 1 # (default: 1)
      # Tensor parallelism involves sharding (horizontally) individual layers of the model into smaller,
      # independent blocks of computation that can be executed on different devices.
      # Attention blocks and multi-layer perceptron (MLP) layers are major components of transformers that can take advantage of
      # tensor parallelism.
      # In multi-head attention blocks, each head or group of heads can be assigned to a different device so they can be computed
      # independently and in parallel.
      tensor: 1 # (default: 1)

# Configuration options for Triton Server.
triton: # (required)
  # Configuration options related to the container image for Triton Server.
  image: # (required)
   # Name of the container image containing the version of Triton Server to be used.
    name: 210086341041.dkr.ecr.us-west-2.amazonaws.com/triton_trtllm_multinode:24.07
  # Configuration options managing the resources assigned to individual Triton Server instances.
  resources: # (optional)
    # Number of logical CPU cores reserved for, and assigned to each instance of Triton Server.
    cpu: 4 # (default: 4)
    # Amount of CPU-visible system memory allocated to, and reserved for each instance of Triton Server.
    memory: 0 # (default: 32Gi)
    # Amount of EFA adapters in your nodes. If you don't want to enable EFA, simply set it to 0.
    efa: 1 # (default: 1)
  triton_model_repo_path: # (required)
  # Enable profiling on Triton server. Note if you send lots of requests, nsys report can be very large.
  enable_nsys: false # (default: false)

# Configuration options related to how various components generate logs.
logging: # (optional)
  # Logging configuration options specific to Triton Server.
  tritonServer:
    # When `true` Triton Server logs are formatted using the ISO8601 standard; otherwise Triton's default format will be used.
    useIso8601: false # (default: false)
    # When `true` Triton Server uses verbose logging; otherwise standard logging is used.
    verbose: false # (default: false)

# Configurations option related to the Kubernetes objects created by the chart.
kubernetes: # (optional)
  # Optional set of labels to be applied to created Kubernetes objects.
  # These labels can be used for association with a preexisting service object.
  labels: # (optional)
    # customLabel: exampleValue
  # When `false`, a service will not be created when the chart is installed; otherwise a service will be created.
  noService: false # (default: false)
  # Name of the service account to use when deploying components.
  # When not provided, a service account will be created.
  serviceAccount: 0 # (optional)
  # Tolerations applied to every pod deployed as part of this deployment.
  # Template already includes `nvidia.com/gpu=present:NoSchedule`.
  tolerations: # (optional)

# Configuration options for automatic scaling of Triton Server deployments.
autoscaling: # (optional)
  # Determines if autoscaling is enabled for deployment or not.
  enable: true # (default: true)
  # Controls the number of Triton Server replicas are deployed.
  replicas: # (optional)
    # Upper bound of the number of Triton Server replicas deployed concurrently.
    maximum: 2 # (default: 4)
    # Lower bound of the number of Triton Server replicas deployed concurrently.
    minimum: 1 # (default: 1)
  # Metric used to determine autoscaling decisions.
  metric: # (optional)
    # Name of the metric monitored.
    name: triton:queue_compute:ratio # (default: triton:queue_compute:ratio)
    # Threshold or targeted value used to determine the number of replicas concurrently deployed.
    value: 1 # (default: 1)
