# generated by fastapi-codegen:
#   filename:  openai_trimmed.yml
#   timestamp: 2024-05-05T21:52:36+00:00

from __future__ import annotations

import argparse
import time
import uuid
from typing import Optional, Union

import numpy
import tritonserver
import uvicorn
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import StreamingResponse
from openai_protocol_types import (
    ChatCompletionChoice,
    ChatCompletionFinishReason,
    ChatCompletionResponseMessage,
    ChatCompletionStreamingResponseChoice,
    ChatCompletionStreamResponseDelta,
    Choice,
    CreateChatCompletionRequest,
    CreateChatCompletionResponse,
    CreateChatCompletionStreamResponse,
    CreateCompletionRequest,
    CreateCompletionResponse,
    FinishReason,
    ListModelsResponse,
    Model,
    ObjectType,
)
from transformers import AutoTokenizer, PreTrainedTokenizer, PreTrainedTokenizerFast
from transformers_utils.tokenizer import get_tokenizer
from triton_cli.constants import SUPPORTED_BACKENDS
from triton_cli.parser import KNOWN_MODEL_SOURCES as KNOWN_MODELS

TIMEOUT_KEEP_ALIVE = 5  # seconds


server: tritonserver.Server
model: tritonserver.Model
model_source_name: str
model_create_time: int
backend: str
tokenizer: Union[PreTrainedTokenizer, PreTrainedTokenizerFast]
create_inference_request = None


def load_model(server):
    model = None
    backends = []
    tokenizer = None
    model_source_name = None
    for model_name, version in server.models().keys():
        if version != -1:
            continue
        current_model = server.load(model_name)
        backends.append(current_model.config()["backend"])
        if model_name in KNOWN_MODELS.keys():
            model = current_model
            model_source_name = KNOWN_MODELS[model_name].replace("hf:", "")
            tokenizer = get_tokenizer(model_source_name)
    if model and tokenizer:
        for backend in backends:
            if backend in SUPPORTED_BACKENDS:
                return model, int(time.time()), backend, tokenizer, model_source_name
    return None, None, None, None, None


app = FastAPI(
    title="OpenAI API",
    description="The OpenAI REST API. Please see https://platform.openai.com/docs/api-reference for more details.",
    version="2.0.0",
    termsOfService="https://openai.com/policies/terms-of-use",
    contact={"name": "OpenAI Support", "url": "https://help.openai.com/"},
    license={
        "name": "MIT",
        "url": "https://github.com/openai/openai-openapi/blob/master/LICENSE",
    },
    servers=[{"url": "https://api.openai.com/v1"}],
)


def get_output(response):
    if "text_output" in response.outputs:
        try:
            return response.outputs["text_output"].to_string_array()[0]
        except:
            return str(response.outputs["text_output"].to_bytes_array()[0])
    return None


def streaming_chat_completion_response(request_id, created, model, role, responses):
    # first chunk

    choice = ChatCompletionStreamingResponseChoice(
        index=0,
        delta=ChatCompletionStreamResponseDelta(
            role=role, content=None, function_call=None
        ),
        logprobs=None,
        finish_reason=None,
    )
    chunk = CreateChatCompletionStreamResponse(
        id=request_id,
        choices=[choice],
        created=created,
        model=model,
        system_fingerprint=None,
        object=ObjectType.chat_completion_chunk,
    )
    yield f"data: {chunk.json(exclude_unset=True)}\n\n"

    for response in responses:
        text = get_output(response)

        choice = ChatCompletionStreamingResponseChoice(
            index=0,
            delta=ChatCompletionStreamResponseDelta(
                role=None, content=text, function_call=None
            ),
            logprobs=None,
            finish_reason=ChatCompletionFinishReason.stop if response.final else None,
        )

        chunk = CreateChatCompletionStreamResponse(
            id=request_id,
            choices=[choice],
            created=created,
            model=model,
            system_fingerprint=None,
            object=ObjectType.chat_completion_chunk,
        )

        yield f"data: {chunk.json(exclude_unset=True)}\n\n"

    yield "data: [DONE]\n\n"


def create_vllm_inference_request(
    model, prompt, request: CreateChatCompletionRequest | CreateCompletionRequest
):
    inputs = {}
    sampling_parameters = request.copy(
        exclude={"model", "stream", "messages", "prompt", "echo"},
    ).model_dump(exclude_none=True)
    inputs["text_input"] = [prompt]
    inputs["stream"] = [request.stream]
    exclude_input_in_output = True
    echo = getattr(request, "echo", None)
    if echo:
        exclude_input_in_output = not echo
    inputs["exclude_input_in_output"] = [exclude_input_in_output]
    return model.create_request(inputs=inputs, parameters=sampling_parameters)


def create_trtllm_inference_request(
    model, prompt, request: CreateChatCompletionRequest | CreateCompletionRequest
):
    inputs = {}
    if model.name == "llama-3-8b-instruct":
        inputs["stop_words"] = [["<|eot_id|>", "<|end_of_text|>"]]
    inputs["text_input"] = [[prompt]]
    inputs["stream"] = [[request.stream]]
    if request.max_tokens:
        inputs["max_tokens"] = numpy.int32([[request.max_tokens]])
    if request.stop:
        if isinstance(request.stop, str):
            request.stop = [request.stop]
        inputs["stop_words"] = [request.stop]
    if request.top_p:
        inputs["top_p"] = numpy.float32([[request.top_p]])
    if request.frequency_penalty:
        inputs["frequency_penalty"] = numpy.float32([[request.frequency_penalty]])
    if request.presence_penalty:
        inputs["presence_penalty":] = numpy.int32([[request.presence_penalty]])
    if request.seed:
        inputs["random_seed"] = numpy.uint64([[request.seed]])
    if request.temperature:
        inputs["temperature"] = numpy.float32([[request.temperature]])

    return model.create_request(inputs=inputs)


@app.post(
    "/v1/chat/completions", response_model=CreateChatCompletionResponse, tags=["Chat"]
)
def create_chat_completion(
    request: CreateChatCompletionRequest,
) -> CreateChatCompletionResponse | StreamingResponse:
    """
    Creates a model response for the given chat conversation.
    """

    if not model or not tokenizer or not create_inference_request:
        raise Exception("Unknown Model")

    add_generation_prompt_default = True
    default_role = "assistant"

    if request.model != model.name and request.model != model_source_name:
        raise HTTPException(status_code=404, detail=f"Unknown model: {request.model}")

    if request.n and request.n > 1:
        raise HTTPException(status_code=400, detail=f"Only single choice is supported")

    conversation = [
        {"role": str(message.role), "content": str(message.content)}
        for message in request.messages
    ]

    prompt = tokenizer.apply_chat_template(
        conversation=conversation,
        tokenize=False,
        add_generation_prompt=add_generation_prompt_default,
    )

    request_id = f"cmpl-{uuid.uuid1()}"
    created = int(time.time())

    responses = model.infer(create_inference_request(model, prompt, request))

    if request.stream:
        return StreamingResponse(
            streaming_chat_completion_response(
                request_id, created, request.model, conversation[-1]["role"], responses
            )
        )

    response = list(responses)[0]

    text = get_output(response)

    return CreateChatCompletionResponse(
        id=request_id,
        choices=[
            ChatCompletionChoice(
                index=0,
                message=ChatCompletionResponseMessage(
                    content=text, role=default_role, function_call=None
                ),
                logprobs=None,
                finish_reason=ChatCompletionFinishReason.stop,
            )
        ],
        created=created,
        model=request.model,
        system_fingerprint=None,
        object=ObjectType.chat_completion,
    )


def streaming_completion_response(request_id, created, model, responses):
    for response in responses:
        text = get_output(response)

        choice = Choice(
            finish_reason=FinishReason.stop if response.final else None,
            index=0,
            logprobs=None,
            text=text,
        )
        response = CreateCompletionResponse(
            id=request_id,
            choices=[choice],
            system_fingerprint=None,
            object=ObjectType.text_completion,
            created=created,
            model=model,
        )

        yield f"data: {response.json(exclude_unset=True)}\n\n"
    yield "data: [DONE]\n\n"


@app.post(
    "/v1/completions", response_model=CreateCompletionResponse, tags=["Completions"]
)
def create_completion(
    request: CreateCompletionRequest, raw_request: Request
) -> CreateCompletionResponse | StreamingResponse:
    """
    Creates a completion for the provided prompt and parameters.
    """

    if not model or not tokenizer or not create_inference_request:
        raise Exception("Unknown Model")

    if request.suffix is not None:
        raise HTTPException(status_code=400, detail="suffix is not currently supported")

    if request.model != model.name and request.model != model_source_name:
        raise HTTPException(status_code=404, detail=f"Unknown model: {request.model}")

    if request.prompt is None:
        request.prompt = "<|endoftext|>"

    # Currently only support single string as input
    if not isinstance(request.prompt, str):
        raise HTTPException(
            status_code=400, detail="only single string input is supported"
        )

    if request.logit_bias is not None or request.logprobs is not None:
        raise HTTPException(
            status_code=400, detail="logit bias and log probs not supported"
        )

    request_id = f"cmpl-{uuid.uuid1()}"
    created = int(time.time())

    responses = model.infer(create_inference_request(model, request.prompt, request))
    if request.stream:
        return StreamingResponse(
            streaming_completion_response(request_id, created, model.name, responses)
        )
    response = list(responses)[0]
    text = get_output(response)

    choice = Choice(
        finish_reason=FinishReason.stop if response.final else None,
        index=0,
        logprobs=None,
        text=text,
    )
    return CreateCompletionResponse(
        id=request_id,
        choices=[choice],
        system_fingerprint=None,
        object=ObjectType.text_completion,
        created=created,
        model=model.name,
    )


owned_by = "ACME"


@app.get("/metrics")
def metrics() -> str:
    return server.metrics()


@app.get("/v1/models", response_model=ListModelsResponse, tags=["Models"])
def list_models() -> ListModelsResponse:
    """
    Lists the currently available models, and provides basic information about each one such as the owner and availability.
    """

    model_list = [
        Model(
            id=model.name,
            created=model_create_time,
            object=ObjectType.model,
            owned_by=owned_by,
        ),
        Model(
            id=model_source_name,
            created=model_create_time,
            object=ObjectType.model,
            owned_by=owned_by,
        ),
    ]

    return ListModelsResponse(object=ObjectType.list, data=model_list)


@app.get("/v1/models/{model_name}", response_model=Model, tags=["Models"])
def retrieve_model(model_name: str) -> Model:
    """
    Retrieves a model instance, providing basic information about the model such as the owner and permissioning.
    """

    if model_name == model.name:
        return Model(
            id=model.name,
            created=model_create_time,
            object=ObjectType.model,
            owned_by=owned_by,
        )

    if model_name == model_source_name:
        return Model(
            id=model_source_name,
            created=model_create_time,
            object=ObjectType.model,
            owned_by=owned_by,
        )

    raise HTTPException(status_code=404, detail=f"Unknown model: {model_name}")


def parse_args():
    parser = argparse.ArgumentParser(
        description="Triton OpenAI Compatible RESTful API server."
    )
    parser.add_argument("--host", type=str, default=None, help="host name")
    parser.add_argument("--port", type=int, default=8000, help="port number")
    parser.add_argument(
        "--uvicorn-log-level",
        type=str,
        default="info",
        choices=["debug", "info", "warning", "error", "critical", "trace"],
        help="log level for uvicorn",
    )
    parser.add_argument(
        "--response-role", type=str, default="assistant", help="The role name to return"
    )

    parser.add_argument(
        "--tritonserver-log-level",
        type=int,
        default=0,
        help="The tritonserver log level",
    )

    parser.add_argument(
        "--model-repository",
        type=str,
        default="/workspace/llm-models",
        help="model repository",
    )
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()

    print("Starting Triton Server Core", flush=True)

    server = tritonserver.Server(
        model_repository=args.model_repository,
        log_verbose=args.tritonserver_log_level,
        strict_model_config=False,
        model_control_mode=tritonserver.ModelControlMode.EXPLICIT,
    ).start(wait_until_ready=True)

    print("Loading Model...\n\n", flush=True)

    model, model_create_time, backend, tokenizer, model_source_name = load_model(server)

    if not (model and backend and tokenizer and model_create_time):
        raise Exception("Unknown Model")

    print(f"\n\nModel: {model.name} Loaded with Backend: {backend}\n\n", flush=True)

    if backend == "vllm":
        create_inference_request = create_vllm_inference_request
    elif backend == "tensorrtllm":
        create_inference_request = create_trtllm_inference_request

    uvicorn.run(
        app,
        host=args.host,
        port=args.port,
        log_level=args.uvicorn_log_level,
        timeout_keep_alive=TIMEOUT_KEEP_ALIVE,
    )
